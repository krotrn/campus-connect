x-nginx-common: &nginx-common
  image: nginx:1.29-alpine
  volumes:
    - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
  networks:
    - campus_connect_net
  healthcheck:
    test:
      [
        "CMD",
        "wget",
        "--quiet",
        "--spider",
        "--fail",
        "http://localhost:8080/nginx-health",
      ]
    interval: 10s
    timeout: 5s
    retries: 3

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

x-dev-build: &dev-build
  build:
    context: .
    dockerfile: Dockerfile
    target: dev

x-dev-env: &dev-env
  env_file:
    - .env.local

x-prod-env: &prod-env
  env_file:
    - .env
    - .env.production

services:
  nginx-dev:
    <<: *nginx-common
    mem_limit: 128m
    cpus: "0.20"
    container_name: campus_connect_nginx_dev
    restart: unless-stopped
    ports:
      - "80:80"
      - "5555:5555"
      - "8080:8080"
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d/dev.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      app-dev:
        condition: service_healthy
      prisma-studio:
        condition: service_healthy
      minio:
        condition: service_healthy
    profiles:
      - dev
    logging: *default-logging

  migrator-dev:
    <<: [*dev-build, *dev-env]
    container_name: campus_connect_migrator_dev
    restart: "no"
    command: pnpm prisma migrate deploy
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - NODE_ENV=development
    networks:
      - campus_connect_net
    depends_on:
      db:
        condition: service_healthy
    mem_limit: 512m
    cpus: "0.50"
    volumes:
      - ./prisma:/app/prisma
      - ./prisma.config.ts:/app/prisma.config.ts
      - node_modules_volume:/app/node_modules
    profiles:
      - dev

  app-dev:
    <<: [*dev-build, *dev-env]
    container_name: campus_connect_app_dev
    restart: unless-stopped
    command: ["pnpm", "dev"]
    depends_on:
      db:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
      create-buckets:
        condition: service_completed_successfully
      migrator-dev:
        condition: service_completed_successfully
    mem_limit: 4g
    cpus: "2.0"
    volumes:
      - .:/app
      - node_modules_volume:/app/node_modules
      - next_cache_volume:/app/.next
      - pnpm_store_volume:/root/.local/share/pnpm/store
    expose:
      - "3000"
    environment:
      - NODE_ENV=development
      - NEXT_TELEMETRY_DISABLED=1
      - CHOKIDAR_USEPOLLING=false
      - WATCHPACK_POLLING=false
      - TZ=UTC
      - REDIS_URL=redis://redis:6379
    networks:
      - campus_connect_net
    healthcheck:
      test: ["CMD", "curl", "-I", "http://localhost:3000/api/health/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - dev
    logging: *default-logging

  worker-dev:
    <<: *dev-build
    container_name: campus_connect_worker_dev
    command: npx tsx workers/index.ts
    volumes:
      - ./workers:/app/workers
      - ./tsconfig.worker.json:/app/tsconfig.worker.json
      - node_modules_volume:/app/node_modules
      - pnpm_store_volume:/root/.local/share/pnpm/store
    environment:
      - REDIS_HOST=redis
      - DATABASE_URL=${DATABASE_URL}
    depends_on:
      redis:
        condition: service_healthy
      migrator-dev:
        condition: service_completed_successfully
    networks:
      - campus_connect_net
    profiles:
      - dev
    logging: *default-logging
    mem_limit: 512m
    cpus: "0.50"

  prisma-studio:
    <<: [*dev-build, *dev-env]
    container_name: campus_connect_prisma_studio
    restart: unless-stopped
    command: pnpm prisma studio --port 5555 --browser none
    depends_on:
      db:
        condition: service_healthy
      migrator-dev:
        condition: service_completed_successfully
    volumes:
      - ./prisma:/app/prisma
      - ./prisma.config.ts:/app/prisma.config.ts
      - ./tsconfig.json:/app/tsconfig.json
      - ./tsconfig.worker.json:/app/tsconfig.worker.json
      - node_modules_volume:/app/node_modules
    expose:
      - "5555"
    environment:
      - NODE_ENV=development
      - TZ=UTC
    mem_limit: 256m
    cpus: "0.30"
    networks:
      - campus_connect_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5555"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - dev
    logging: *default-logging

  nginx-prod:
    <<: *nginx-common
    container_name: campus_connect_nginx_prod
    restart: always
    ports:
      - "80:80"
      - "8080:8080"
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d/prod.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      app-prod:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
      worker-prod:
        condition: service_healthy
    profiles:
      - prod
    mem_limit: 128m
    cpus: "0.25"
    logging: *default-logging

  migrator-prod:
    <<: *prod-env
    build:
      context: .
      dockerfile: Dockerfile
      target: migrator
    container_name: campus_connect_migrator_prod
    restart: on-failure:5
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - NODE_ENV=production
    networks:
      - campus_connect_net
    depends_on:
      db:
        condition: service_healthy
    mem_limit: 256m
    cpus: "0.25"
    profiles:
      - prod
    logging: *default-logging

  app-prod:
    <<: *prod-env
    build:
      context: .
      dockerfile: Dockerfile
      target: runner
    container_name: campus_connect_app_prod
    restart: always
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      migrator-prod:
        condition: service_completed_successfully
    expose:
      - "3000"
    environment:
      - TZ=UTC
      - HOSTNAME=0.0.0.0
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
    networks:
      - campus_connect_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:3000/api/health/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - prod
    mem_limit: 3g
    mem_reservation: 1g
    cpus: "1.5"
    cpu_shares: 1024
    stop_grace_period: 30s
    logging: *default-logging

  worker-prod:
    <<: *prod-env
    build:
      context: .
      dockerfile: Dockerfile
      target: worker-runner
    container_name: campus_connect_worker_prod
    restart: always
    environment:
      - REDIS_HOST=redis
      - DATABASE_URL=${DATABASE_URL}
      - NODE_ENV=production
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
      migrator-prod:
        condition: service_completed_successfully
    networks:
      - campus_connect_net
    profiles:
      - prod
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'node dist/workers/index.js' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    mem_limit: 1g
    mem_reservation: 512m
    cpus: "1.0"
    cpu_shares: 768
    stop_grace_period: 60s
    logging: *default-logging

  db:
    image: postgres:18.1-alpine
    container_name: campus_connect_db
    restart: always
    env_file:
      - .env
    expose:
      - "5432"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U connect -d campus_connect"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - campus_connect_net
    profiles:
      - dev
      - prod
    mem_limit: 2g
    cpus: "1.0"
    shm_size: 256mb
    logging: *default-logging

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    container_name: campus_connect_minio
    restart: always
    command: server --console-address ":9001" /data
    env_file:
      - .env
    expose:
      - "9000"
      - "9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - campus_connect_net
    profiles:
      - dev
      - prod
    mem_limit: 512m
    cpus: "0.5"
    logging: *default-logging

  create-buckets:
    image: minio/mc
    container_name: campus_connect_mc
    pull_policy: if_not_present
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
        /usr/bin/mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
        /usr/bin/mc mb myminio/${NEXT_PUBLIC_MINIO_BUCKET} --ignore-existing;
      "
    env_file:
      - .env
    networks:
      - campus_connect_net
    profiles:
      - dev
      - prod

  redis:
    image: redis:8.2.1-alpine
    container_name: campus_connect_redis
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 400mb --maxmemory-policy noeviction
    expose:
      - "6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - campus_connect_net
    profiles:
      - dev
      - prod
    mem_limit: 512m
    cpus: "0.5"
    logging: *default-logging

  prometheus-prod:
    image: prom/prometheus:v3.9.1
    container_name: campus_connect_prometheus_prod
    restart: always
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--web.enable-lifecycle"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules.yml:/etc/prometheus/rules.yml:ro
      - prometheus_data:/prometheus
    expose:
      - "9090"
    networks:
      - campus_connect_net
    depends_on:
      - cadvisor
      - node-exporter
      - alertmanager
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:9090/-/healthy",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - prod
    mem_limit: 512m
    cpus: "0.5"
    logging: *default-logging

  alertmanager:
    image: prom/alertmanager:v0.28.1
    container_name: campus_connect_alertmanager
    restart: always
    entrypoint: ["/alertmanager-entrypoint.sh"]
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml.template:/etc/alertmanager/alertmanager.yml.template:ro
      - ./monitoring/alertmanager/alertmanager-entrypoint.sh:/alertmanager-entrypoint.sh:ro
      - alertmanager_data:/alertmanager
    environment:
      - SMTP_HOST=${SMTP_HOST:-smtp.gmail.com}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - ALERT_EMAIL_FROM=${ALERT_EMAIL_FROM}
      - ALERT_EMAIL_TO=${ALERT_EMAIL_TO}
    expose:
      - "9093"
    networks:
      - campus_connect_net
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:9093/-/healthy",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - prod
    mem_limit: 64m
    cpus: "0.25"
    logging: *default-logging

  grafana:
    image: grafana/grafana:12.3.2
    container_name: campus_connect_grafana
    restart: always
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost/grafana
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-piechart-panel
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
    expose:
      - "3000"
    networks:
      - campus_connect_net
    depends_on:
      - loki
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:3000/api/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - prod
    mem_limit: 256m
    cpus: "0.5"
    logging: *default-logging

  loki:
    image: grafana/loki:3.6.4
    container_name: campus_connect_loki
    restart: always
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./monitoring/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    expose:
      - "3100"
    networks:
      - campus_connect_net
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:3100/ready",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - prod
    mem_limit: 256m
    cpus: "0.5"
    logging: *default-logging

  promtail:
    image: grafana/promtail:3.6.4
    container_name: campus_connect_promtail
    restart: always
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./monitoring/promtail/promtail-config.yaml:/etc/promtail/config.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - promtail_positions:/var/lib/promtail
    networks:
      - campus_connect_net
    depends_on:
      - loki
    profiles:
      - prod
    mem_limit: 64m
    cpus: "0.25"
    logging: *default-logging

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.55.1
    container_name: campus_connect_cadvisor
    restart: always
    privileged: true
    devices:
      - /dev/kmsg:/dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command:
      - "--docker_only=true"
      - "--store_container_labels=true"
      - "--housekeeping_interval=10s"
    expose:
      - "8080"
    networks:
      - campus_connect_net
    profiles:
      - prod
    mem_limit: 128m
    cpus: "0.25"
    logging: *default-logging

  node-exporter:
    image: prom/node-exporter:v1.8.2
    container_name: campus_connect_node_exporter
    restart: always
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/rootfs"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc|mnt/wsl)($$|/)"
      - "--no-collector.softnet"
      - "--no-collector.netstat"
      - "--no-collector.mdadm"
      - "--no-collector.infiniband"
      - "--no-collector.nfs"
      - "--no-collector.nfsd"
      - "--no-collector.filesystem"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    expose:
      - "9100"
    networks:
      - campus_connect_net
    profiles:
      - prod
    mem_limit: 32m
    cpus: "0.1"
    logging: *default-logging

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.18.1
    container_name: campus_connect_postgres_exporter
    restart: always
    environment:
      - DATA_SOURCE_NAME=postgresql://${POSTGRES_USER:-connect}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB:-campus_connect}?sslmode=disable
    expose:
      - "9187"
    networks:
      - campus_connect_net
    depends_on:
      db:
        condition: service_healthy
    profiles:
      - prod
    mem_limit: 32m
    cpus: "0.1"
    logging: *default-logging

  redis-exporter:
    image: oliver006/redis_exporter:v1.80.2
    container_name: campus_connect_redis_exporter
    restart: always
    environment:
      - REDIS_ADDR=redis://redis:6379
    expose:
      - "9121"
    networks:
      - campus_connect_net
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - prod
    mem_limit: 32m
    cpus: "0.1"
    logging: *default-logging

  uptime-kuma:
    image: louislam/uptime-kuma:1
    container_name: campus_connect_uptime_kuma
    restart: always
    volumes:
      - uptime_kuma_data:/app/data
    expose:
      - "3001"
    networks:
      - campus_connect_net
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "require('http').get('http://localhost:3001/api/health', r => process.exit(r.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - prod
    mem_limit: 256m
    cpus: "0.5"
    logging: *default-logging

networks:
  campus_connect_net:
    driver: bridge
    ipam:
      config:
        - subnet: 10.12.0.0/16

volumes:
  postgres_data:
  minio_data:
  redis_data:
  node_modules_volume:
  next_cache_volume:
  pnpm_store_volume:
  prometheus_data:
  alertmanager_data:
  grafana_data:
  loki_data:
  uptime_kuma_data:
  promtail_positions:
